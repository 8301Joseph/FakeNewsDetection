# FakeNewsDetection
Group 6 Fake News Detection - NLP
Project Name: Fake News Detection
Total Sprints: 3
Project Manager: Benjamin Yan

Team Members:
Cheri Ho, Iurii Beliaev, James Wright, Evan "Reese" Pagtalunan, Joseph Glasson, Kevin Yao

Project Description:
Train a model to detect whether a given sentence or tweet is fake news or not.
Potential to expand the scope toward a research focus (improving state-of-the-art).

## Getting Started

### 1) Clone the repository (skip if already cloned)
```bash
git clone REPLACE_WITH_YOUR_REPO_URL
cd FakeNewsDetection
```

### 2) Initialize Git LFS for this repository
**Important**: Git LFS must be initialized in the repository before you can access large CSV files.

```bash
# Initialize Git LFS for this repository (required for accessing large CSV files)
git lfs install
```

**Note**: If you see "Git LFS is not installed for this repository" when running `git lfs pull`, you need to run `git lfs install` first. This sets up Git LFS hooks for this specific repository.

### 3) Create and activate the Conda environment
- The environment name is `fake-news-env` (from `environment.yml`).
```bash
conda env create -f environment.yml
conda activate fake-news-env
```

If the environment already exists, just activate it:
```bash
conda activate fake-news-env
```

To update dependencies later:
```bash
conda env update -f environment.yml --prune
```

**Important**: After creating the environment, download NLTK data:
```bash
# Download required NLTK data (required for lemmatization.py)
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')"
```

### 4) Pull Git LFS files
After cloning and initializing Git LFS, download the actual CSV file contents:

```bash
# Pull all Git LFS files (CSV files are stored in Git LFS)
git lfs pull

# Or pull specific files
git lfs pull --include="lemmatized.csv"
git lfs pull --include="WELFake_Dataset.csv"
git lfs pull --include="*.csv"  # Pull all CSV files
```

**Troubleshooting**: 
- If you get "Skipping object checkout, Git LFS is not installed for this repository", run `git lfs install` first
- If files appear as small pointer files (~134 bytes), run `git lfs pull` to download the actual content

To leave the environment:
```bash
conda deactivate
```

### 5) Update local files after repository changes

If someone else has updated the repository and you've pulled the latest changes:

```bash
# Pull latest code changes
git pull

# Pull Git LFS files (if CSV files were updated)
git lfs pull

# Update conda environment if dependencies changed
conda env update -f environment.yml --prune
```

**Note**: Large CSV files (`*.csv`) are stored in Git LFS to reduce repository size. After pulling changes, run `git lfs pull` to download the actual file contents. If you see files as small pointer files (~134 bytes), that means you need to run `git lfs pull` to download the real content.

## Project Files Overview

This section explains the purpose and contents of each file in the repository.

### üìä Data Files

#### `WELFake_Dataset.csv`
- **Purpose**: Main dataset containing fake and real news articles
- **Source**: WELFake dataset from Kaggle (https://www.kaggle.com/datasets/saurabhshahane/fake-news-classification)
- **Format**: CSV with columns including `text` (article content) and `label` (0=real, 1=fake)
- **Note**: Stored in Git LFS due to large file size (~245MB)

#### `lemmatized.csv`
- **Purpose**: Preprocessed dataset output from `lemmatization.py`
- **Contents**: Original dataset with an additional `Cleaned text` column containing lemmatized, cleaned text
- **Usage**: Used as input for TF-IDF, Doc2Vec, Word2Vec, and Sentence2Vec processing

#### `doc2vec_output.csv`
- **Purpose**: Dataset with Doc2Vec embeddings added
- **Contents**: Original columns plus `doc2vec` column containing vector embeddings for each document
- **Generated by**: `doc2vec.py`
- **Note**: Stored in Git LFS due to large file size

#### `word2vec_output.csv`
- **Purpose**: Dataset with Word2Vec embeddings added
- **Contents**: Original columns plus `word_vectors` column containing list of word-level vector embeddings (one vector per word)
- **Generated by**: `word2vec.py`
- **Note**: Similar structure to Sentence2Vec output (multiple vectors per document)

#### `train_set.csv` & `test_set.csv`
- **Purpose**: Train/test splits created from Doc2Vec output
- **Contents**: 80% training data and 20% testing data (stratified split)
- **Generated by**: `traintest.py`
- **Note**: Stored in Git LFS due to large file size

### üêç Python Scripts

#### `lemmatization.py`
- **Purpose**: Text preprocessing and cleaning pipeline
- **Functions**:
  - `clean_lemmatize()`: Performs comprehensive text cleaning including:
    - Lowercasing
    - Removal of non-alphabetic characters
    - Tokenization
    - Stopword removal (preserves negation words like "no", "not")
    - Part-of-speech tagging
    - Lemmatization (using WordNet POS tags)
  - `to_wn_pos()`: Converts NLTK POS tags to WordNet format
  - `process_dataset()`: Applies cleaning to entire dataset and saves to `lemmatized.csv`
- **Dependencies**: pandas, nltk (punkt, stopwords, wordnet), re
- **Output**: Creates `lemmatized.csv` with cleaned text

#### `TF-IDF.py`
- **Purpose**: Complete TF-IDF (Term Frequency-Inverse Document Frequency) vectorization pipeline
- **Features**:
  - Loads and preprocesses data using `lemmatization.py`
  - Applies TF-IDF vectorization with configurable parameters (max_features=5000, ngrams, etc.)
  - Analyzes top features distinguishing fake vs real news
  - Identifies most distinctive words between classes
  - Creates stratified train/test split (80/20)
  - Saves vectorizer, feature names, and TF-IDF matrix for later use
- **Output Files**:
  - `tfidf_vectorizer.pkl`: Fitted vectorizer object
  - `tfidf_feature_names.npy`: Feature names array
  - `tfidf_matrix.npz`: Sparse TF-IDF matrix
- **Dependencies**: pandas, numpy, scikit-learn, matplotlib, seaborn
- **See also**: `TF-IDF_README.md` for detailed usage instructions

#### `doc2vec.py`
- **Purpose**: Generates document embeddings using Doc2Vec (Gensim)
- **Process**:
  1. Loads lemmatized data using `scripts/convert_csv_to_df.py`
  2. Creates TaggedDocument objects for training
  3. Trains Doc2Vec model (vector_size=50, min_count=2, epochs=20)
  4. Generates document vectors for each article
  5. Saves results to `doc2vec_output.csv`
- **Dependencies**: gensim, pandas
- **Output**: `doc2vec_output.csv` with Doc2Vec embeddings

#### `sentence2vec.py`
- **Purpose**: Converts sentences to vectors using the sent2vec library
- **Process**:
  1. Splits cleaned text into individual sentences
  2. Generates vector embeddings for each sentence using sent2vec
  3. Stores sentence vectors in the dataframe
- **Functions**:
  - `split_into_sentences()`: Splits text by sentence boundaries (. ! ?)
  - `sentence_list_to_vectors()`: Converts sentence lists to vector embeddings
  - `process_lemmatized()`: Main processing function
- **Dependencies**: pandas, sent2vec, re
- **Output**: `sentence2vec.csv` (if run)

#### `word2vec.py`
- **Purpose**: Generates word-level embeddings using Word2Vec (Gensim)
- **Process**:
  1. Loads lemmatized data using `scripts/convert_csv_to_df.py`
  2. Trains Word2Vec model on tokenized documents (vector_size=50, min_count=2, epochs=20)
  3. Generates word vectors for each word in each document
  4. Stores word vectors as lists (one vector per word) in the dataframe
- **Functions**:
  - `word_list_to_vectors()`: Converts list of words to list of word vectors
  - `get_word_vectors_for_document()`: Wrapper function to get word vectors for a document
- **Dependencies**: gensim, pandas
- **Output**: `word2vec_output.csv` with word-level vectors (similar structure to Sentence2Vec)

#### `traintest.py`
- **Purpose**: Creates train/test splits from Doc2Vec output
- **Process**:
  1. Loads `doc2vec_output.csv`
  2. Splits data into 80% train / 20% test (stratified, random_state=42)
  3. Saves separate CSV files for training and testing
- **Dependencies**: pandas, scikit-learn
- **Output**: `train_set.csv` and `test_set.csv`

### üõ†Ô∏è Utility Scripts

#### `scripts/convert_csv_to_df.py`
- **Purpose**: Utility function to convert CSV files to pandas DataFrames with tokenization
- **Functions**:
  - `csv_to_df()`: Converts CSV to DataFrame, adds ID column, and tokenizes specified text columns
  - `tokenize_text()`: Simple tokenizer that splits text by whitespace
- **Usage**: Imported by other scripts (e.g., `doc2vec.py`) to load and tokenize data
- **Dependencies**: pandas

### üìù Configuration Files

#### `environment.yml`
- **Purpose**: Conda environment specification for Mac/Linux
- **Contents**: Complete dependency list including:
  - Python 3.12
  - Data science libraries (numpy, pandas, scikit-learn, scipy)
  - NLP libraries (gensim, nltk)
  - Jupyter ecosystem (jupyter, jupyterlab, notebook, ipykernel)
  - Visualization (matplotlib, matplotlib-inline, seaborn)
  - Web scraping (beautifulsoup4, requests)
  - Git LFS (git-lfs) - Required for accessing large CSV files stored in Git LFS
  - sent2vec (via pip) - Required for sentence2vec.py
- **Usage**: `conda env create -f environment.yml`
- **After installation**: 
  - Run `git lfs install` in the repository directory to initialize Git LFS for this repo
  - Run `git lfs pull` to download large CSV files
  - Download NLTK data: `python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')"`

#### `environment_windows.yml`
- **Purpose**: Simplified Conda environment specification for Windows
- **Contents**: Core dependencies including:
  - Python 3.12
  - Data science libraries (numpy, pandas, scikit-learn, scipy)
  - NLP libraries (gensim, nltk)
  - Jupyter ecosystem (jupyter, jupyterlab, notebook, ipykernel)
  - Visualization (matplotlib, matplotlib-inline, seaborn)
  - Git LFS (git-lfs)
  - sent2vec (via pip)
- **Usage**: Alternative environment file for Windows users
- **After installation**: 
  - Run `git lfs install` in the repository directory to initialize Git LFS for this repo
  - Run `git lfs pull` to download large CSV files
  - Download NLTK data: `python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')"`

#### `requirements.txt`
- **Purpose**: Python package requirements (pip format)
- **Status**: Currently empty (dependencies managed via Conda)

### üìö Documentation Files

#### `README.md` (this file)
- **Purpose**: Main project documentation
- **Contents**: Project overview, setup instructions, sprint plans, and file descriptions

#### `TF-IDF_README.md`
- **Purpose**: Detailed documentation for the TF-IDF pipeline
- **Contents**: 
  - Quick start guide
  - Explanation of TF-IDF process
  - Feature analysis details
  - Customization options
  - Expected output examples

## Workflow Overview

The typical data processing pipeline follows this order:

1. **Data Preprocessing**: `lemmatization.py` ‚Üí `lemmatized.csv`
2. **Feature Extraction** (choose one or more):
   - **TF-IDF**: `TF-IDF.py` ‚Üí TF-IDF matrices and analysis
   - **Doc2Vec**: `doc2vec.py` ‚Üí `doc2vec_output.csv`
   - **Word2Vec**: `word2vec.py` ‚Üí `word2vec_output.csv`
   - **Sentence2Vec**: `sentence2vec.py` ‚Üí `sentence2vec.csv`
3. **Train/Test Split**: `traintest.py` ‚Üí `train_set.csv`, `test_set.csv`
4. **Model Training**: Use the prepared datasets with machine learning classifiers
