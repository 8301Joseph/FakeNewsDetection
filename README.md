# FakeNewsDetection
Group 6 Fake News Detection - NLP
Project Name: Fake News Detection
Total Sprints: 3
Project Manager: Benjamin Yan

Team Members:
Cheri Ho, Iurii Beliaev, James Wright, Evan "Reese" Pagtalunan, Joseph Glasson, Kevin Yao

Project Description:
Train a model to detect whether a given sentence or tweet is fake news or not.
Potential to expand the scope toward a research focus (improving state-of-the-art).

## Getting Started

### 1) Clone the repository (skip if already cloned)
```bash
git clone REPLACE_WITH_YOUR_REPO_URL
cd FakeNewsDetection
```

### 2) Create and activate the Conda environment
- The environment name is `fake-news-env` (from `environment.yml`).
```bash
conda env create -f environment.yml
conda activate fake-news-env
```

If the environment already exists, just activate it:
```bash
conda activate fake-news-env
```

To update dependencies later:
```bash
conda env update -f environment.yml --prune
```

To leave the environment:
```bash
conda deactivate
```

## Project Files Overview

This section explains the purpose and contents of each file in the repository.

### üìä Data Files

#### `WELFake_Dataset.csv`
- **Purpose**: Main dataset containing fake and real news articles
- **Source**: WELFake dataset from Kaggle (https://www.kaggle.com/datasets/saurabhshahane/fake-news-classification)
- **Format**: CSV with columns including `text` (article content) and `label` (0=real, 1=fake)
- **Note**: Stored in Git LFS due to large file size (~245MB)

#### `lemmatized.csv`
- **Purpose**: Preprocessed dataset output from `lemmatization.py`
- **Contents**: Original dataset with an additional `Cleaned text` column containing lemmatized, cleaned text
- **Usage**: Used as input for TF-IDF, Doc2Vec, and Sentence2Vec processing

#### `doc2vec_output.csv`
- **Purpose**: Dataset with Doc2Vec embeddings added
- **Contents**: Original columns plus `doc2vec` column containing vector embeddings for each document
- **Generated by**: `doc2vec.py`
- **Note**: Stored in Git LFS due to large file size

#### `train_set.csv` & `test_set.csv`
- **Purpose**: Train/test splits created from Doc2Vec output
- **Contents**: 80% training data and 20% testing data (stratified split)
- **Generated by**: `traintest.py`
- **Note**: Stored in Git LFS due to large file size

### üêç Python Scripts

#### `lemmatization.py`
- **Purpose**: Text preprocessing and cleaning pipeline
- **Functions**:
  - `clean_lemmatize()`: Performs comprehensive text cleaning including:
    - Lowercasing
    - Removal of non-alphabetic characters
    - Tokenization
    - Stopword removal (preserves negation words like "no", "not")
    - Part-of-speech tagging
    - Lemmatization (using WordNet POS tags)
  - `to_wn_pos()`: Converts NLTK POS tags to WordNet format
  - `process_dataset()`: Applies cleaning to entire dataset and saves to `lemmatized.csv`
- **Dependencies**: pandas, nltk (punkt, stopwords, wordnet), re
- **Output**: Creates `lemmatized.csv` with cleaned text

#### `TF-IDF.py`
- **Purpose**: Complete TF-IDF (Term Frequency-Inverse Document Frequency) vectorization pipeline
- **Features**:
  - Loads and preprocesses data using `lemmatization.py`
  - Applies TF-IDF vectorization with configurable parameters (max_features=5000, ngrams, etc.)
  - Analyzes top features distinguishing fake vs real news
  - Identifies most distinctive words between classes
  - Creates stratified train/test split (80/20)
  - Saves vectorizer, feature names, and TF-IDF matrix for later use
- **Output Files**:
  - `tfidf_vectorizer.pkl`: Fitted vectorizer object
  - `tfidf_feature_names.npy`: Feature names array
  - `tfidf_matrix.npz`: Sparse TF-IDF matrix
- **Dependencies**: pandas, numpy, scikit-learn, matplotlib, seaborn
- **See also**: `TF-IDF_README.md` for detailed usage instructions

#### `doc2vec.py`
- **Purpose**: Generates document embeddings using Doc2Vec (Gensim)
- **Process**:
  1. Loads lemmatized data using `scripts/convert_csv_to_df.py`
  2. Creates TaggedDocument objects for training
  3. Trains Doc2Vec model (vector_size=50, min_count=2, epochs=20)
  4. Generates document vectors for each article
  5. Saves results to `doc2vec_output.csv`
- **Dependencies**: gensim, pandas
- **Output**: `doc2vec_output.csv` with Doc2Vec embeddings

#### `sentence2vec.py`
- **Purpose**: Converts sentences to vectors using the sent2vec library
- **Process**:
  1. Splits cleaned text into individual sentences
  2. Generates vector embeddings for each sentence using sent2vec
  3. Stores sentence vectors in the dataframe
- **Functions**:
  - `split_into_sentences()`: Splits text by sentence boundaries (. ! ?)
  - `sentence_list_to_vectors()`: Converts sentence lists to vector embeddings
  - `process_lemmatized()`: Main processing function
- **Dependencies**: pandas, sent2vec, re
- **Output**: `sentence2vec.csv` (if run)

#### `traintest.py`
- **Purpose**: Creates train/test splits from Doc2Vec output
- **Process**:
  1. Loads `doc2vec_output.csv`
  2. Splits data into 80% train / 20% test (stratified, random_state=42)
  3. Saves separate CSV files for training and testing
- **Dependencies**: pandas, scikit-learn
- **Output**: `train_set.csv` and `test_set.csv`

### üõ†Ô∏è Utility Scripts

#### `scripts/convert_csv_to_df.py`
- **Purpose**: Utility function to convert CSV files to pandas DataFrames with tokenization
- **Functions**:
  - `csv_to_df()`: Converts CSV to DataFrame, adds ID column, and tokenizes specified text columns
  - `tokenize_text()`: Simple tokenizer that splits text by whitespace
- **Usage**: Imported by other scripts (e.g., `doc2vec.py`) to load and tokenize data
- **Dependencies**: pandas

### üìù Configuration Files

#### `environment.yml`
- **Purpose**: Conda environment specification for Mac/Linux
- **Contents**: Complete dependency list including:
  - Python 3.12
  - Data science libraries (numpy, pandas, scikit-learn, scipy)
  - Jupyter ecosystem (jupyter, jupyterlab, notebook, ipykernel)
  - Visualization (matplotlib-inline)
  - Web scraping (beautifulsoup4, requests)
- **Usage**: `conda env create -f environment.yml`

#### `environment_windows.yml`
- **Purpose**: Simplified Conda environment specification for Windows
- **Contents**: Core dependencies only (Python 3.12, numpy, pandas, scikit-learn, scipy, jupyter, etc.)
- **Usage**: Alternative environment file for Windows users

#### `requirements.txt`
- **Purpose**: Python package requirements (pip format)
- **Status**: Currently empty (dependencies managed via Conda)

### üìö Documentation Files

#### `README.md` (this file)
- **Purpose**: Main project documentation
- **Contents**: Project overview, setup instructions, sprint plans, and file descriptions

#### `TF-IDF_README.md`
- **Purpose**: Detailed documentation for the TF-IDF pipeline
- **Contents**: 
  - Quick start guide
  - Explanation of TF-IDF process
  - Feature analysis details
  - Customization options
  - Expected output examples

## Workflow Overview

The typical data processing pipeline follows this order:

1. **Data Preprocessing**: `lemmatization.py` ‚Üí `lemmatized.csv`
2. **Feature Extraction** (choose one or more):
   - **TF-IDF**: `TF-IDF.py` ‚Üí TF-IDF matrices and analysis
   - **Doc2Vec**: `doc2vec.py` ‚Üí `doc2vec_output.csv`
   - **Sentence2Vec**: `sentence2vec.py` ‚Üí `sentence2vec.csv`
3. **Train/Test Split**: `traintest.py` ‚Üí `train_set.csv`, `test_set.csv`
4. **Model Training**: Use the prepared datasets with machine learning classifiers

------------------------------------------------------------
Milestone / Sprint 1: Setup, Data Curation, and EDA
------------------------------------------------------------

WEEK 1 ‚Äì Setup Environment
------------------------------------------------------------
Goal: Initial setup and project structure

Task 1: Environment Setup #1
Description: Create a Conda environment and local directory for the project
Work Hours: 1

Task 2: Environment Setup #2
Description: Create a shared GitHub repository with a requirements.txt file
Work Hours: 2

Task 3: Install GitHub Repo
Description: Clone the GitHub repository and install dependencies via pip
Work Hours: 1

Task 4: Download & Load Data
Description: Download fake news dataset from:
https://www.kaggle.com/datasets/saurabhshahane/fake-news-classification
Work Hours: 2


WEEK 2 ‚Äì Text Processing
------------------------------------------------------------
Goal: Preprocess and clean text data

Task 1: Convert from Dict to DataFrame
Description: Convert dictionary to pandas DataFrame for easier handling.
Each row = a post; columns = ID, tokens (optionally concatenated), and most common label from annotators.
Work Hours: 2‚Äì3

Task 2: Preprocess & Clean Text Data
Description: Perform stopword removal, punctuation removal, stemming, and lemmatization.
Work Hours: 1‚Äì2

Task 3: TF-IDF
Description: Learn and apply TF-IDF (Term Frequency‚ÄìInverse Document Frequency) for text representation.
Work Hours: 2‚Äì3


WEEK 3 ‚Äì Text Vectorization
------------------------------------------------------------
Goal: Represent text numerically for model input

Task 1: Understand Word2Vec
Description: Learn how Word2Vec converts words into vectors.
Work Hours: 1‚Äì2

Task 2: Implement Sentence2Vec / Doc2Vec
Description: Use Sentence2Vec or Doc2Vec to embed full sentences and compare performance.
Work Hours: 1‚Äì2

Task 3: Buffer Time
Description: Catch up on delayed tasks or unforeseen issues.
Work Hours: 2‚Äì3


------------------------------------------------------------
Milestone / Sprint 2: Data Training
------------------------------------------------------------

WEEK 4 ‚Äì Training a Machine Learning Model
------------------------------------------------------------
Goal: Build and evaluate baseline models

Task 1: Prepare Dataset
Description: Perform feature engineering and split into train/test sets.
Work Hours: 1‚Äì2

Task 2: Choose Classifier
Description: Select a model (kNN, Random Forest, XGBoost, LSTM, etc.)
Research strengths, weaknesses, and assumptions of each.
Work Hours: 3‚Äì4

Task 3: Train Model
Description: Train the model using scikit-learn, TensorFlow, or other libraries.
Work Hours: 1‚Äì2

Task 4: Evaluate Classifier
Description: Assess model performance across different classes.
Work Hours: 1‚Äì2


WEEKS 5‚Äì6 ‚Äì BERT
------------------------------------------------------------
Goal: Fine-tune transformer-based models for improved accuracy

Task 1: Understand BERT
Description: Study BERT (see http://jalammar.github.io/illustrated-bert/) and why it could help in fake news detection.
Work Hours: 2‚Äì3

Task 2: Fine-Tune BERT
Description: Fine-tune a pre-trained BERT model on fake news data.
Work Hours: 5‚Äì6

Task 3: Compare Approaches
Description: Compare results from traditional ML vs. BERT.
Work Hours: 1‚Äì2

Task 4: Room for Improvement
Description: Explore research papers or new methods for improving fake news detection.
Work Hours: 3‚Äì4


------------------------------------------------------------
WEEK 7 and Beyond ‚Äì Optional Extensions
------------------------------------------------------------
- Build a simple web app to classify input text as fake or real.
- Conduct R&D to improve model performance.
- Implement Explainable AI (XAI) for interpretability.
- Experiment with better encoding methods.
- Write a Medium blog post summarizing findings.

------------------------------------------------------------
